# User configuration for Asimi CLI
# This file would typically be located at ~/.config/asimi/conf.toml

[server]
host = "0.0.0.0"
port = 8080

[database]
host = "db.example.com"
port = 5432
user = "production_user"
password = "production_password"
name = "asimi_prod"

[logging]
level = "error"
format = "json"

# LLM configuration examples:
# For OpenAI:
# [llm]
# provider = "openai"
# model = "gpt-4"
# api_key = "your-openai-api-key"
# base_url = ""

# For Ollama (local models):
# [llm]
# provider = "ollama"
# model = "llama3"
# api_key = ""
# base_url = ""

# For Google Gemini:
# [llm]
# provider = "gemini"
# model = "gemini-pro"
# api_key = "your-gemini-api-key"
# base_url = ""

# For Anthropic:
# [llm]
# provider = "anthropic"
# model = "claude-3-5-sonnet-20240620"
# api_key = "your-anthropic-api-key"
# base_url = ""
# cleanup_period_days = 30
# include_coauthored_by = true
# auto_updates = true
# preferred_notif_channel = "iterm2"
# verbose = false
# max_mcp_output_tokens = 25000

# Permission settings examples:
# [permission]
# allow = []
# ask = []
# deny = []
# additional_directories = []

# Hooks settings examples:
# [hooks]
# pre_tool = []
# post_tool = []

# Status line settings examples:
# [statusline]
# enabled = false